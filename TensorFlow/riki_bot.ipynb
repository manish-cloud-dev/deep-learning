{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "riki_botv4.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qurh3Re5GA1A",
        "colab_type": "text"
      },
      "source": [
        "# Download Cornell Corpus and Glove Dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "puCdGhwBoE2T",
        "colab_type": "text"
      },
      "source": [
        "### set enable_twitter_glove_download to true to fetch dataset from internet. ~1.4Gb"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a-AyHaTunySk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "109f9196-87d5-40ed-83fb-0811d9698058"
      },
      "source": [
        "import numpy as np\n",
        "import os\n",
        "import sys\n",
        "import zipfile\n",
        "import urllib.request\n",
        "from collections import Counter\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Enable only if want to download entire set else use github\n",
        "enable_twitter_glove_download = False\n",
        "\n",
        "GLOVE_EMBEDDING_SIZE = 25\n",
        "GLOVE_VECTOR = \"/content/data_corpus/glove.twitter.27B.\" + str(GLOVE_EMBEDDING_SIZE) + \"d.txt\"\n",
        "CORNELL_CORPUS = \"/content/data_corpus/cornell movie-dialogs corpus/movie_conversations.txt\"\n",
        "MAX_SEQUENCE_LENGTH=200\n",
        "MAX_NB_WORDS=4000\n",
        "\n",
        "BATCH_SIZE = 64  ##increasing batchsize causing crash on colab\n",
        "# with last minute changes can't get time to complete more epocs in so logs are for 1 epoch .ipynb\n",
        "NUM_EPOCHS = 1  \n",
        "HIDDEN_UNITS = 256\n",
        "MAX_INPUT_SEQ_LENGTH = 40\n",
        "MAX_TARGET_SEQ_LENGTH = 40\n",
        "MAX_VOCAB_SIZE = 10000\n",
        "\n",
        "WEIGHT_FILE_PATH = 'models/'+ '/word-glove-weights.h5'\n",
        "\n",
        "if not os.path.exists('./models'):\n",
        "  os.makedirs('./models')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oNUWcv-cybt_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import requests\n",
        "\n",
        "def download_file_from_google_drive(id, destination):\n",
        "    def get_confirm_token(response):\n",
        "        for key, value in response.cookies.items():\n",
        "            if key.startswith('download_warning'):\n",
        "                return value\n",
        "\n",
        "        return None\n",
        "\n",
        "    def save_response_content(response, destination):\n",
        "        CHUNK_SIZE = 32768\n",
        "\n",
        "        with open(destination, \"wb\") as f:\n",
        "            for chunk in response.iter_content(CHUNK_SIZE):\n",
        "                if chunk: # filter out keep-alive new chunks\n",
        "                    f.write(chunk)\n",
        "\n",
        "    URL = \"https://docs.google.com/uc?export=download\"\n",
        "\n",
        "    session = requests.Session()\n",
        "\n",
        "    response = session.get(URL, params = { 'id' : id }, stream = True)\n",
        "    token = get_confirm_token(response)\n",
        "    \n",
        "    if token:\n",
        "        params = { 'id' : id, 'confirm' : token }\n",
        "        response = session.get(URL, params = params, stream = True)\n",
        "    print(response)\n",
        "    save_response_content(response, destination)    \n",
        "\n",
        "#download_file_from_google_drive(\"1cv-2rNKLZPttBrliwPrtQO9dwR0qSOw8\",\"/content/data_corpus/glove.twitter.27B.25d.txt\")"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "hxLfGh8En3Dq",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "74975acf-05cd-4934-c4a7-d385c655cc0f"
      },
      "source": [
        "def download_corpus(CORPUS, path, zip_file, url):\n",
        "  if not os.path.exists(CORPUS):\n",
        "    if not os.path.exists(path):\n",
        "        os.makedirs(path)\n",
        "\n",
        "    if not os.path.exists(zip_file):\n",
        "        print('corpus file not exist, downloading from net')\n",
        "        urllib.request.urlretrieve(url=url, filename=zip_file)\n",
        "\n",
        "    zip_ref = zipfile.ZipFile(zip_file, 'r')\n",
        "    zip_ref.extractall(path)\n",
        "    zip_ref.close()\n",
        "\n",
        "download_corpus(CORPUS=CORNELL_CORPUS,  path='/content/data_corpus',\n",
        "      zip_file='/content/data_corpus/cornell_movie_dialogs_corpus.zip',\n",
        "      url='http://www.cs.cornell.edu/~cristian/data/cornell_movie_dialogs_corpus.zip')\n",
        "\n",
        "if not os.path.exists(CORNELL_CORPUS):\n",
        "  print(\"Download cornell corpus failed...!!\")\n",
        "else:\n",
        "  print(\"cornell corpus downloaded.\")\n",
        "\n",
        "if enable_twitter_glove_download:\n",
        "  download_corpus(CORPUS=GLOVE_VECTOR,  path='/content/data_corpus',\n",
        "        zip_file='/content/data_corpus/glove.twitter.27B.zip',\n",
        "        url='http://nlp.stanford.edu/data/glove.twitter.27B.zip')\n",
        "else:\n",
        "  download_file_from_google_drive(\"1cv-2rNKLZPttBrliwPrtQO9dwR0qSOw8\",\"/content/data_corpus/glove.twitter.27B.25d.txt\")\n",
        "  #print(\"Using github for glove dataset\")\n",
        "  #from google.colab import drive\n",
        "  #drive.mount('/content/gdrive')\n",
        "  #GLOVE_VECTOR = '/content/gdrive/My Drive/Colab Notebooks/glove.twitter.27B.25d.txt'\n",
        "\n",
        "\n",
        "if not os.path.exists(GLOVE_VECTOR):\n",
        "  print(\"Download glove corpus failed...!!\")\n",
        "else:\n",
        "  print(\"glove corpus downloaded.\")\n",
        "\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cornell corpus downloaded.\n",
            "<Response [200]>\n",
            "glove corpus downloaded.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xDtwGinCGOYy",
        "colab_type": "text"
      },
      "source": [
        "# Process Cornell Movie Corpus Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E6D-sU4fjasX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load the data\n",
        "CORNELL_LINE_CORPUS = \"/content/data_corpus/cornell movie-dialogs corpus/movie_lines.txt\"\n",
        "CORNELL_CONV_CORPUS =  CORNELL_CORPUS\n",
        "lines = open(CORNELL_LINE_CORPUS, encoding='utf-8', errors='ignore').read().split('\\n')\n",
        "conv_lines = open(CORNELL_CONV_CORPUS, encoding='utf-8', errors='ignore').read().split('\\n')\n",
        "\n",
        "#LINE: ['L1045 +++$+++ u0 +++$+++ m0 +++$+++ BIANCA +++$+++ They do not!']\n",
        "#CONV: [\"u0 +++$+++ u2 +++$+++ m0 +++$+++ ['L194', 'L195', 'L196', 'L197']\"]\n",
        "\n",
        "#Build VOCAB from movie_lines\n",
        "#Build  Question & Answer set from conv to train as X,Y\n",
        "\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dgD4a2FIksN8",
        "colab_type": "text"
      },
      "source": [
        "###### Based on many online chatbot, many used this to filter the text, its really good"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OjBdBnmFRfXw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import re\n",
        "def clean_text(text):\n",
        "    '''Clean text by removing unnecessary characters and altering the format of words.'''\n",
        "\n",
        "    text = text.lower()\n",
        "    \n",
        "    text = re.sub(r\"i'm\", \"i am\", text)\n",
        "    text = re.sub(r\"he's\", \"he is\", text)\n",
        "    text = re.sub(r\"she's\", \"she is\", text)\n",
        "    text = re.sub(r\"it's\", \"it is\", text)\n",
        "    text = re.sub(r\"that's\", \"that is\", text)\n",
        "    text = re.sub(r\"what's\", \"that is\", text)\n",
        "    text = re.sub(r\"where's\", \"where is\", text)\n",
        "    text = re.sub(r\"how's\", \"how is\", text)\n",
        "    text = re.sub(r\"\\'ll\", \" will\", text)\n",
        "    text = re.sub(r\"\\'ve\", \" have\", text)\n",
        "    text = re.sub(r\"\\'re\", \" are\", text)\n",
        "    text = re.sub(r\"\\'d\", \" would\", text)\n",
        "    text = re.sub(r\"\\'re\", \" are\", text)\n",
        "    text = re.sub(r\"won't\", \"will not\", text)\n",
        "    text = re.sub(r\"can't\", \"cannot\", text)\n",
        "    text = re.sub(r\"n't\", \" not\", text)\n",
        "    text = re.sub(r\"n'\", \"ng\", text)\n",
        "    text = re.sub(r\"'bout\", \"about\", text)\n",
        "    text = re.sub(r\"'til\", \"until\", text)\n",
        "    text = re.sub(r\"[-()\\\"#/@;:<>{}`+=~|.!?,]\", \"\", text)\n",
        "    \n",
        "    return text\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HpxGEL5GQz_t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create a dictionary to map each line's id with its text\n",
        "texts = []\n",
        "for line in lines:\n",
        "    _line = line.split(' +++$+++ ')\n",
        "    if len(_line) == 5:\n",
        "        texts.append(clean_text(_line[4]))\n",
        "\n",
        "with open('clean_movie_file.txt', 'w') as fp:\n",
        "    for stmt in texts:\n",
        "        fp.write('%s\\n' % stmt)\n",
        "lines = open('clean_movie_file.txt', 'r').read().split('\\n')"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AF2L7hpniP4B",
        "colab_type": "text"
      },
      "source": [
        "####1. Load the glove word embedding into a dictionary. [10 Marks]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mj7uzUZwHGPX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "9470450e-3edd-483e-d493-6be5208c1b9e"
      },
      "source": [
        "def load_glove():\n",
        "  word2em = {}\n",
        "  with open(GLOVE_VECTOR) as fp:\n",
        "    for line in fp:\n",
        "      words = line.strip().split()\n",
        "      word = words[0]\n",
        "      vecs = np.asarray(words[1:], dtype='float32')\n",
        "      word2em[word] = vecs\n",
        "\n",
        "  print('Found %s word vectors.' % len(word2em))\n",
        "  return word2em\n",
        "\n",
        "word2em = load_glove()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 1193514 word vectors.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QGToOwVElQ4C",
        "colab_type": "text"
      },
      "source": [
        "####Data Preparation - Filter the conversations till max word length and convert the dialogues pairs into input text and target texts. Mark start and end token to recognize the beginning and end of the sentence token"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u8UaNVIA-LhQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "target_counter = Counter()\n",
        "input_texts = []\n",
        "target_texts = []\n",
        "prev_words = []\n",
        "\n",
        "for line in lines:\n",
        "    next_words = [w.lower() for w in nltk.word_tokenize(line)]\n",
        "    if len(next_words) > MAX_TARGET_SEQ_LENGTH:\n",
        "        next_words = next_words[0:MAX_TARGET_SEQ_LENGTH]\n",
        "\n",
        "    if len(prev_words) > 0:\n",
        "        input_texts.append(prev_words)\n",
        "        target_words = next_words[:]\n",
        "        target_words.insert(0, 'start')\n",
        "        target_words.append('end')\n",
        "\n",
        "        for w in target_words:\n",
        "            target_counter[w] += 1\n",
        "        target_texts.append(target_words)\n",
        "\n",
        "    prev_words = next_words\n"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jrVPHXidleZU",
        "colab_type": "text"
      },
      "source": [
        "####2. Create a target word to id dictionary called target_word2idx. [10 Marks]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nzv4mvkgldGJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "target_word2idx = dict()\n",
        "for idx, word in enumerate(target_counter.most_common(MAX_VOCAB_SIZE)):\n",
        "    target_word2idx[word[0]] = idx + 1\n",
        "\n",
        "if 'unknown' not in target_word2idx:\n",
        "    target_word2idx['unknown'] = 0\n",
        "\n",
        "np.save('models/' + '/word-glove-target-word2idx.npy', target_word2idx)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M3nB-kP6i44O",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "####3. Create a target to id dictionary called target_idx2word. [10 Marks]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uvuxHjari4dH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "target_idx2word = dict([(idx, word) for word, idx in target_word2idx.items()])\n",
        "np.save('models/' + '/word-glove-target-idx2word.npy', target_idx2word)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qin7d95imAcC",
        "colab_type": "text"
      },
      "source": [
        "#### Number of unique decoder tokens"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r5hSJ6Ryl-CJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "num_decoder_tokens = len(target_idx2word)+1"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vh_DDc5YjDIU",
        "colab_type": "text"
      },
      "source": [
        "####4. Prepare the input data with embedding. [15 Marks]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "79sz5bsEBB7Q",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "771d98a8-8c22-4486-fe7a-e950a1cdda83"
      },
      "source": [
        "input_texts_word2em = []\n",
        "\n",
        "encoder_max_seq_length = 0\n",
        "decoder_max_seq_length = 0\n",
        "\n",
        "for input_words, target_words in zip(input_texts, target_texts):\n",
        "    encoder_input_wids = []\n",
        "    for w in input_words:\n",
        "        emb = np.zeros(shape=GLOVE_EMBEDDING_SIZE)\n",
        "        if w in word2em:\n",
        "            emb = word2em[w]\n",
        "        encoder_input_wids.append(emb)\n",
        "\n",
        "    input_texts_word2em.append(encoder_input_wids)\n",
        "    encoder_max_seq_length = max(len(encoder_input_wids), encoder_max_seq_length)\n",
        "    decoder_max_seq_length = max(len(target_words), decoder_max_seq_length)\n",
        "\n",
        "context = dict()\n",
        "context['num_decoder_tokens'] = num_decoder_tokens\n",
        "context['encoder_max_seq_length'] = encoder_max_seq_length\n",
        "context['decoder_max_seq_length'] = decoder_max_seq_length\n",
        "\n",
        "print(context)\n",
        "np.save('models/' + '/word-glove-context.npy', context)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'num_decoder_tokens': 10001, 'encoder_max_seq_length': 40, 'decoder_max_seq_length': 42}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m2Gn5PAYjJ7o",
        "colab_type": "text"
      },
      "source": [
        "####5. Generate training data per batch. [15 Marks]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eulmBSsgCAJt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generate_batch(input_word2em_data, output_text_data):\n",
        "    num_batches = len(input_word2em_data) // BATCH_SIZE\n",
        "    while True:\n",
        "        for batchIdx in range(0, num_batches):\n",
        "            start = batchIdx * BATCH_SIZE\n",
        "            end = (batchIdx + 1) * BATCH_SIZE\n",
        "            encoder_input_data_batch = pad_sequences(input_word2em_data[start:end], encoder_max_seq_length)\n",
        "            decoder_target_data_batch = np.zeros(shape=(BATCH_SIZE, decoder_max_seq_length, num_decoder_tokens))\n",
        "            decoder_input_data_batch = np.zeros(shape=(BATCH_SIZE, decoder_max_seq_length, GLOVE_EMBEDDING_SIZE))\n",
        "            for lineIdx, target_words in enumerate(output_text_data[start:end]):\n",
        "                for idx, w in enumerate(target_words):\n",
        "                    w2idx = target_word2idx['unknown']  \n",
        "                    if w in target_word2idx:\n",
        "                        w2idx = target_word2idx[w]\n",
        "                    if w in word2em:\n",
        "                        decoder_input_data_batch[lineIdx, idx, :] = word2em[w]\n",
        "                    if idx > 0:\n",
        "                        decoder_target_data_batch[lineIdx, idx - 1, w2idx] = 1\n",
        "            yield [encoder_input_data_batch, decoder_input_data_batch], decoder_target_data_batch\n",
        "\n"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EN2ABMxbCcbv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "e2e286f1-9610-4300-b500-f06c1baeb8f8"
      },
      "source": [
        "from keras.models import Model\n",
        "from keras.layers.recurrent import LSTM\n",
        "from keras.layers import Dense, Input, Embedding\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from collections import Counter\n",
        "import nltk\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "isplDdOBjPrB",
        "colab_type": "text"
      },
      "source": [
        "####6. Defining correct model architecture. [10 Marks]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "By6m3H_2CPVl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "7a80f29b-c3fe-4bf6-d199-af0de26bc607"
      },
      "source": [
        "encoder_inputs = Input(shape=(None, GLOVE_EMBEDDING_SIZE), name='encoder_inputs')\n",
        "encoder_lstm = LSTM(units=HIDDEN_UNITS, return_state=True, name='encoder_lstm')\n",
        "encoder_outputs, encoder_state_h, encoder_state_c = encoder_lstm(encoder_inputs)\n",
        "encoder_states = [encoder_state_h, encoder_state_c]\n",
        "\n",
        "decoder_inputs = Input(shape=(None, GLOVE_EMBEDDING_SIZE), name='decoder_inputs')\n",
        "decoder_lstm = LSTM(units=HIDDEN_UNITS, return_state=True, return_sequences=True, name='decoder_lstm')\n",
        "decoder_outputs, decoder_state_h, decoder_state_c = decoder_lstm(decoder_inputs,\n",
        "                                                                 initial_state=encoder_states)\n",
        "decoder_dense = Dense(units=num_decoder_tokens, activation='softmax', name='decoder_dense')\n",
        "decoder_outputs = decoder_dense(decoder_outputs)\n",
        "\n",
        "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n",
        "\n",
        "json = model.to_json()\n",
        "open('models/'+ '/word-glove-architecture.json', 'w').write(json)\n",
        "\n",
        "Xtrain, Xtest, Ytrain, Ytest = train_test_split(input_texts_word2em, target_texts, test_size=0.2, random_state=42)\n",
        "\n",
        "print(len(Xtrain))\n",
        "print(len(Xtest))\n"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "243552\n",
            "60888\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I81KxssaCq0o",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "00982809-e081-473f-cfe4-b1ec20fde6b8"
      },
      "source": [
        "train_gen = generate_batch(Xtrain, Ytrain)\n",
        "test_gen = generate_batch(Xtest, Ytest)\n",
        "\n",
        "train_num_batches = len(Xtrain) // BATCH_SIZE\n",
        "test_num_batches = len(Xtest) // BATCH_SIZE\n",
        "\n",
        "checkpoint = ModelCheckpoint(filepath=WEIGHT_FILE_PATH, save_best_only=True)\n",
        "model.fit_generator(generator=train_gen, steps_per_epoch=train_num_batches,\n",
        "                    epochs=NUM_EPOCHS,\n",
        "                    verbose=1, validation_data=test_gen, validation_steps=test_num_batches, callbacks=[checkpoint])\n",
        "\n",
        "model.save_weights(WEIGHT_FILE_PATH)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/1\n",
            "3805/3805 [==============================] - 1537s 404ms/step - loss: 1.3872 - val_loss: 1.3306\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r5u1OOxO8JSw",
        "colab_type": "text"
      },
      "source": [
        "## Test Chatbot"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kpWmt634mPrH",
        "colab_type": "text"
      },
      "source": [
        "#### To filter noise in input text for prediction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mcr54FHGNd-s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "WHITELIST = 'abcdefghijklmnopqrstuvwxyz1234567890?.,'\n",
        "\n",
        "def in_white_list(_word):\n",
        "    for char in _word:\n",
        "        if char in WHITELIST:\n",
        "            return True\n",
        "\n",
        "    return False"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MPB1V4x-mXZa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# glbal variables\n",
        "model = None\n",
        "encoder_model = None\n",
        "decoder_model = None\n",
        "target_word2idx = None\n",
        "target_idx2word = None\n",
        "max_decoder_seq_length = None\n",
        "max_encoder_seq_length = None\n",
        "num_decoder_tokens = None"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J4w6Mfa0IIDE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Initialize the chatbot for prediction\n",
        "\n",
        "target_word2idx = np.load('./models/' + '/word-glove-target-word2idx.npy', allow_pickle=True).item()\n",
        "target_idx2word = np.load('./models/' + '/word-glove-target-idx2word.npy', allow_pickle=True).item()\n",
        "context = np.load('./models/' + '/word-glove-context.npy', allow_pickle=True).item()\n",
        "max_encoder_seq_length = context['encoder_max_seq_length']\n",
        "max_decoder_seq_length = context['decoder_max_seq_length']\n",
        "num_decoder_tokens = context['num_decoder_tokens']\n",
        "\n",
        "encoder_inputs = Input(shape=(None, GLOVE_EMBEDDING_SIZE), name='encoder_inputs')\n",
        "encoder_lstm = LSTM(units=HIDDEN_UNITS, return_state=True, name=\"encoder_lstm\")\n",
        "encoder_outputs, encoder_state_h, encoder_state_c = encoder_lstm(encoder_inputs)\n",
        "encoder_states = [encoder_state_h, encoder_state_c]\n",
        "\n",
        "decoder_inputs = Input(shape=(None, GLOVE_EMBEDDING_SIZE), name='decoder_inputs')\n",
        "decoder_lstm = LSTM(units=HIDDEN_UNITS, return_sequences=True, return_state=True, name='decoder_lstm')\n",
        "decoder_outputs, _, _ = decoder_lstm(decoder_inputs, initial_state=encoder_states)\n",
        "decoder_dense = Dense(num_decoder_tokens, activation='softmax', name='decoder_dense')\n",
        "decoder_outputs = decoder_dense(decoder_outputs)\n",
        "\n",
        "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
        "model.load_weights('./models/' + '/word-glove-weights.h5')\n",
        "model.compile(optimizer='rmsprop', loss='categorical_crossentropy')\n",
        "\n",
        "encoder_model = Model(encoder_inputs, encoder_states)\n",
        "decoder_state_inputs = [Input(shape=(HIDDEN_UNITS,)), Input(shape=(HIDDEN_UNITS,))]\n",
        "decoder_outputs, state_h, state_c = decoder_lstm(decoder_inputs, initial_state=decoder_state_inputs)\n",
        "decoder_states = [state_h, state_c]\n",
        "decoder_outputs = decoder_dense(decoder_outputs)\n",
        "decoder_model = Model([decoder_inputs] + decoder_state_inputs, [decoder_outputs] + decoder_states)\n",
        "\n",
        "\n"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PfNM83_ZJqMM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def testme(input_text):\n",
        "    input_seq = []\n",
        "    input_emb = []\n",
        "\n",
        "    # create embedding vector for input text\n",
        "    for word in nltk.word_tokenize(input_text.lower()):\n",
        "        if not in_white_list(word):\n",
        "            continue\n",
        "        emb = np.zeros(shape=GLOVE_EMBEDDING_SIZE)\n",
        "        if word in word2em:\n",
        "            emb = word2em[word]\n",
        "        input_emb.append(emb)\n",
        "    \n",
        "    input_seq.append(input_emb)\n",
        "    input_seq = pad_sequences(input_seq, max_encoder_seq_length)\n",
        "\n",
        "    # use padded input sequence with encoder\n",
        "    states_value = encoder_model.predict(input_seq)\n",
        "    target_seq = np.zeros((1, 1, GLOVE_EMBEDDING_SIZE))\n",
        "    target_seq[0, 0, :] = word2em['start']\n",
        "    target_text = ''\n",
        "    target_text_len = 0\n",
        "    terminated = False\n",
        "    \n",
        "    ## predict the target text/ or repsonse\n",
        "    while not terminated:\n",
        "        output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n",
        "\n",
        "        sample_token_idx = np.argmax(output_tokens[0, -1, :])\n",
        "        sample_word = target_idx2word[sample_token_idx]\n",
        "        target_text_len += 1\n",
        "\n",
        "        if sample_word != 'start' and sample_word != 'end':\n",
        "            target_text += ' ' + sample_word\n",
        "\n",
        "        if sample_word == 'end' or target_text_len >= max_decoder_seq_length:\n",
        "            terminated = True\n",
        "\n",
        "        target_seq = np.zeros((1, 1, GLOVE_EMBEDDING_SIZE))\n",
        "        if sample_word in word2em:\n",
        "            target_seq[0, 0, :] = word2em[sample_word]\n",
        "\n",
        "        states_value = [h, c]\n",
        "    return target_text.strip()\n"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TP-09BioKOsB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "ce48bd8c-5963-4aae-e040-1f99f93d8d56"
      },
      "source": [
        "# unit test \n",
        "print(testme('can we talk?'))"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "i am not going to be a little unknown\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gLoZ7E9NkaHf",
        "colab_type": "text"
      },
      "source": [
        "####7. Final prediction. [20 Marks]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Opz22I8Bg0d1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 399
        },
        "outputId": "5aae0f6a-ada3-4c28-8760-3d42e7c9027a"
      },
      "source": [
        "#Test 10 times\n",
        "print(\"***** LOOP 10 times *******\")\n",
        "for _ in range(10):\n",
        "    print(testme(input('Enter question : ')))\n"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "***** LOOP 10 times *******\n",
            "Enter question : Hello Computer\n",
            "i am not going to be a unknown\n",
            "Enter question : How are you?\n",
            "i am not going to be a little unknown\n",
            "Enter question : who are you?\n",
            "i am not going to be a little unknown\n",
            "Enter question : party?\n",
            "i am not going to be a unknown\n",
            "Enter question : hello\n",
            "i am not going to be a unknown\n",
            "Enter question : hi\n",
            "i am not going to be a unknown\n",
            "Enter question : unknown\n",
            "i am not going to be a unknown\n",
            "Enter question :  bye\n",
            "i am not going to be a unknown\n",
            "Enter question : by bye\n",
            "i am not going to be a unknown\n",
            "Enter question : bye\n",
            "i am not going to be a unknown\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oKC-tmwS2h9I",
        "colab_type": "text"
      },
      "source": [
        "### Synopsis\n",
        "The chatbot data preprocessing an modelling is learned. However, model is performing poorly on cornell dataset. Overall its good learning.\n",
        "Needs optimization in dataset."
      ]
    }
  ]
}